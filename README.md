# GRADAR-NIDS


## Abstract
In the field of IOT (Internet of Things), computing systems and devices are being deployed on the edge of the internet by the day. As a result, the demand for the security of such large systems is becoming a higher priority and this paper will serve to produce an efficient model that can be easily deployed to edge devices and provide an additional layer of security that's so strongly required in this growing industry.

Our goal in particular is to solve a threat classification problem in regards to large computer/edge-device networks. A Network Intrusion Detection System (NIDS) must be able to perform an industry-level accurate ($\geq96 \%$ success rate) evaluation of a transmission's threat level, and be compact enough to be deployable to any number of edge devices, which have constrained computation limits. To achieve this accuracy, I have compiled an LSTM-CNN based deep-learning model that takes in a sequence of actions on a given network, and outputs a binary classifier that evaluates whether or not this sequence contributes to any malicious activity that might possibly harm the performance of the network, or extract private data from it. We are performing post-training integer quantization and GPU-optimization in CoLab in regards to the training process in order to be able to deploy this model to edge devices so that it can run within the constrained resources of edge devices such as Raspberry PIs, and so that the model can still run on outdated devices without significant ($\rho \leq 0.005$) loss of accuracy. In addition, in order to reduce the effects of hindrances such as the Vanishing Gradient problem and the Curse of Dimensionality, we have used a feature-selection model using a Random Forest Classifier to deterministically find the 10 most significant features within our dataset, which was the CSE-CIC-IDS2018 dataset.

GRADAR (short for GPU-backed Radar) is a Network Intrusion Detection System (NIDS) that can analyze packets that have been sniffed on a network (via a packet sniffing tool such as Wireshark), and have features extracted from the open source biflow generator Cicflowmeter-v3. Afterwards, the GRADAR can run an inference on the sniffed sample and detect whether or not there may have been significant malicious activity recorded on the given network. Since IoT devices are getting more and more mainstream, there is a call to enact certain measures of security with regards to these massive networks of IoT devices, such as your home router or even your smart fridge. IoT devices run on compute-limited resources and hence, it can be quite hard to implement on IoT devices. The model we have proposed is an architecture that is able to extract data from temporal relationships between the sample, and also map/inference other features. We implemented a timeseries-inspired approach using 2 LSTM (Long short-term memory) layers that act as a time series analyzer for sequential packets, such as those that have appeared in DoS/DDoS attacks and Denial of Sleep attacks on IoT devices.

For our dataset, in order to take into account multiple modern scenarios of network attacks on large IoT and cloud networks, we have utilized the CSE-CIC-IDS2018 dataset. Combined across samples of packets measured from February 14 over to March 2nd 2018, this dataset has approximately 16 million network samples each with ground-truth labels. We were able to over/undersample underrepresented attack samples and shortened our dataset so that we can maintain a high industry standard of >90% accuracy on both test AND train data. Furthermore, we want to have a reasonably sized dataset to train on so that our inferences are not prone to overfitting, and so we tried to quantize/prune the model effectively and reduce the number of epochs that we have to train it. Overall, our final results were a 98% accuracy on both test and train data, and fair generalization to other cloud/IoT network data that we can extract features from using the [CiCFlowMeter](https://github.com/datthinh1801/cicflowmeter/tree/mainCICFlowMeter) open source tool.

The interesting idea behind our model is that unlike standard models that utilize approahces such as ANN/Autoencoders like [Kitsune](https://github.com/ymirsky/Kitsune-py) and other approaches, outlined below:
![Image](https://cdn.discordapp.com/attachments/819417070185480202/1103160766338699274/image.png)

For next steps, we plan to implement optimized inferencing and generalized model prediction using other frameworks closer to the metal over python such as Rust/C(++). Stay tuned for more :)


